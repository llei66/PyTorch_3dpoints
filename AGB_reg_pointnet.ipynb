{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"name":"AGB_reg_pointnet.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"mOyrT-lfxrNd","executionInfo":{"status":"ok","timestamp":1622122055948,"user_tz":-480,"elapsed":856,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["## init envi if local\n","#!pip install torch==1.5.1+cu92 torchvision==0.6.1+cu92 -f https://download.pytorch.org/whl/torch_stable.html\n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Um2Qof6Hx4Am","executionInfo":{"status":"ok","timestamp":1622122056975,"user_tz":-480,"elapsed":44,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}},"outputId":"8a23a03b-781c-43ab-e0bc-4dbb752c8389"},"source":["## use colab\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxF8BoKmx-JC","executionInfo":{"status":"ok","timestamp":1622122056976,"user_tz":-480,"elapsed":34,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}},"outputId":"b0f92b70-09e8-4d54-8d47-a6853e1a02c6"},"source":["!ls gdrive/MyDrive/DEEPCROP_Lei/reg/reg_pytorch_lidar\n","os.chdir('/content/gdrive/MyDrive/DEEPCROP_Lei/reg/reg_pytorch_lidar/')\n","!pwd"],"execution_count":23,"outputs":[{"output_type":"stream","text":["ls: cannot access 'gdrive/MyDrive/DEEPCROP_Lei/reg/reg_pytorch_lidar': No such file or directory\n","/content/gdrive/MyDrive/DEEPCROP_Lei/reg/reg_pytorch_lidar\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"StioUdoI1jj3"},"source":["###  step by step"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-oXF8q_xrNg","executionInfo":{"status":"ok","timestamp":1622122056977,"user_tz":-480,"elapsed":22,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}},"outputId":"aae08a5c-6de5-4437-e33e-0876a3476637"},"source":["# dataloader for regression\n","import os\n","import numpy as np\n","from torch.utils.data import Dataset\n","import pdb\n","import random\n","import pandas as pd\n","from sklearn.neighbors import NearestNeighbors\n","import matplotlib.pyplot as plt\n","\n","#Standardize the entire data set before dividing it\n","\n","# Standardize the training level before applying the rules to the test set (TODO)\n","\n","class GeoData_crop_1(Dataset):\n","    def __init__(self, split='train', data_root='trainval_fullarea', num_point=4096,  block_size=1.0,  transform=None):\n","        super().__init__()\n","        self.num_point = num_point\n","        self.block_size = block_size\n","        self.transform = transform\n","        rooms = sorted(os.listdir(data_root))\n","        rooms_split = sorted(os.listdir(data_root))\n","\n","        self.room_points, self.room_labels = [], []\n","        self.room_coord_min, self.room_coord_max = [], []\n","        num_point_all = []\n","        self.room_points_1, self.room_labels_1 = [], []\n","        self.intensity_all = []\n","        # for room_name in rooms_split:\n","        #     room_path = os.path.join(data_root, room_name)\n","        #     # room_data = np.load(room_path)  # xyzrgbl, N*7\n","        #     room_data = pd.read_csv(room_path, sep=\"\\s+\").values  # xyzrgbl, N*7\n","        #     # print(room_path)\n","\n","        #     points, labels = room_data[:, 0:3], room_data[:, -1] \n","        #     rgb = room_data[:,10:13]\n","        #     intensity = room_data[:,3].reshape(points.shape[0], 1)\n","        #     # self.intensity_all.append(intensity.tolist())\n","        #     coord_min, coord_max = np.amin(points, axis=0)[:3], np.amax(points, axis=0)[:3]\n","        #     rgb = rgb / (256 * 255)\n","        #     self.room_coord_min.append(coord_min), self.room_coord_max.append(coord_max)\n","        #     points = np.concatenate((points, rgb, intensity), axis=1)\n","\n","        #     self.room_points_1.append(points)\n","        #     for i in labels.tolist():\n","        #         self.room_labels_1.append(i)\n","        #     for j in intensity.tolist():\n","        #         self.intensity_all.append(j)\n","                \n","        # whole_coord_min, whole_coord_max = np.amin(self.room_coord_min, axis=0)[:3], np.amax(self.room_coord_max, axis=0)[:3]\n","        # room_scale = (whole_coord_max - whole_coord_min)\n","        # # normalize intensity\n","        # intensity_mean = np.array(self.intensity_all).mean()\n","        # intensity_std = np.array(self.intensity_all).std()\n","\n","        # # normalize label\n","        # room_labels_mean = np.array(self.room_labels_1).mean()\n","        # room_labels_std = np.array(self.room_labels_1).std()\n","\n","        ### whole data \n","        intensity_mean = 30.015351566593313\n","        intensity_std = 2.2168713538563276\n","\n","        # normalize label\n","        room_labels_mean = 925.8869876854762\n","        room_labels_std =  499.07046037234073\n","        for room_name in rooms_split:\n","            room_path = os.path.join(data_root, room_name)\n","            room_data = pd.read_csv(room_path, sep=\"\\s+\").values  # xyzrgbl, N*7\n","\n","            points, labels = room_data[:, 0:3], room_data[:, -1]\n","            rgb = room_data[:, 10:13]\n","            intensity = room_data[:, 3].reshape(points.shape[0], 1)\n","\n","            intensity = (intensity - intensity_mean) / intensity_std\n","            labels = (labels - room_labels_mean) / room_labels_std\n","\n","            # normalize\n","            points = (points - whole_coord_min) / room_scale\n","            rgb = rgb / (256 * 255)\n","\n","            # self.room_coord_min.append(coord_min), self.room_coord_max.append(coord_max)\n","            num_point_all.append(labels.size)\n","#             points = np.concatenate((points, rgb, intensity), axis=1)\n","            self.room_points.append(points), self.room_labels.append(labels)        # normalize rgb\n","\n","        room_idxs = []\n","        for index in range(len(rooms_split)):\n","            room_idxs.extend([index])\n","        self.room_idxs = np.array(room_idxs)\n","        print(\"Totally {} samples in {} set.\".format(len(self.room_idxs), split))\n","\n","\n","    def __getitem__(self, idx):\n","        room_idx = self.room_idxs[idx]\n","        points = self.room_points[room_idx]   # N * 6\n","        labels = self.room_labels[room_idx]   # N\n","        N_points = points.shape[0]\n","        \n","        return points, labels\n","\n","    def __len__(self):\n","        return len(self.room_idxs)\n","\n","print(\"load dataloader methods\")\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["load dataloader methods\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyCDAgLsxrNi","executionInfo":{"status":"ok","timestamp":1622122056977,"user_tz":-480,"elapsed":16,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}},"outputId":"ce2507fd-6171-4e4c-9a39-8a4cde77b4f2"},"source":["# model init\n","## the basic network\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.utils.data\n","from torch.autograd import Variable\n","import numpy as np\n","import torch.nn.functional as F\n","\n","\n","class STN3d(nn.Module):\n","    def __init__(self, channel):\n","        super(STN3d, self).__init__()\n","        # self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n","        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n","\n","        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n","        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n","        self.fc1 = nn.Linear(1024, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, 9)\n","        self.relu = nn.ReLU()\n","\n","        # self.bn1 = nn.BatchNorm1d(64)\n","        # self.bn2 = nn.BatchNorm1d(128)\n","        # self.bn3 = nn.BatchNorm1d(1024)\n","        # self.bn4 = nn.BatchNorm1d(512)\n","        # self.bn5 = nn.BatchNorm1d(256)\n","\n","    def forward(self, x):\n","        batchsize = x.size()[0]\n","        # x = F.relu(self.bn1(self.conv1(x)))\n","        # x = F.relu(self.bn2(self.conv2(x)))\n","        # x = F.relu(self.bn3(self.conv3(x)))\n","        # x = torch.max(x, 2, keepdim=True)[0]\n","        # x = x.view(-1, 1024)\n","        #\n","        # x = F.relu(self.bn4(self.fc1(x)))\n","        # x = F.relu(self.bn5(self.fc2(x)))\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = torch.max(x, 2, keepdim=True)[0]\n","        x = x.view(-1, 1024)\n","\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        iden = Variable(torch.from_numpy(np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).astype(np.float32))).view(1, 9).repeat(\n","            batchsize, 1)\n","        if x.is_cuda:\n","            iden = iden.cuda()\n","        x = x + iden\n","        x = x.view(-1, 3, 3)\n","        return x\n","\n","\n","class STNkd(nn.Module):\n","    def __init__(self, k=64):\n","        super(STNkd, self).__init__()\n","        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n","        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n","        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n","        self.fc1 = nn.Linear(1024, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, k * k)\n","        self.relu = nn.ReLU()\n","\n","        # self.bn1 = nn.BatchNorm1d(64)\n","        # self.bn2 = nn.BatchNorm1d(128)\n","        # self.bn3 = nn.BatchNorm1d(1024)\n","        # self.bn4 = nn.BatchNorm1d(512)\n","        # self.bn5 = nn.BatchNorm1d(256)\n","\n","        self.k = k\n","\n","    def forward(self, x):\n","        batchsize = x.size()[0]\n","        # x = F.relu(self.bn1(self.conv1(x)))\n","        # x = F.relu(self.bn2(self.conv2(x)))\n","        # x = F.relu(self.bn3(self.conv3(x)))\n","        # x = torch.max(x, 2, keepdim=True)[0]\n","        # x = x.view(-1, 1024)\n","        #\n","        # x = F.relu(self.bn4(self.fc1(x)))\n","        # x = F.relu(self.bn5(self.fc2(x)))\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = torch.max(x, 2, keepdim=True)[0]\n","        x = x.view(-1, 1024)\n","\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1, self.k * self.k).repeat(\n","            batchsize, 1)\n","        if x.is_cuda:\n","            iden = iden.cuda()\n","        x = x + iden\n","        x = x.view(-1, self.k, self.k)\n","        return x\n","\n","\n","class PointNetEncoder(nn.Module):\n","    def __init__(self, global_feat=True, feature_transform=False, channel=3):\n","        super(PointNetEncoder, self).__init__()\n","        self.stn = STN3d(channel)\n","        self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n","        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n","        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n","        # self.bn1 = nn.BatchNorm1d(64)\n","        # self.bn2 = nn.BatchNorm1d(128)\n","        # self.bn3 = nn.BatchNorm1d(1024)\n","        self.global_feat = global_feat\n","        self.feature_transform = feature_transform\n","        if self.feature_transform:\n","            self.fstn = STNkd(k=64)\n","\n","    def forward(self, x):\n","        B, D, N = x.size()\n","\n","        # print(x.size())\n","        trans = self.stn(x)\n","\n","        x = x.transpose(2, 1)\n","        if D > 3:\n","            # x, feature = x.split(3, dim=2)\n","            x, feature = x.split((3,4), dim=2)\n","\n","        x = torch.bmm(x, trans)\n","        if D > 3:\n","            x = torch.cat([x,feature], dim=2)\n","        x = x.transpose(2, 1)\n","        # x = F.relu(self.bn1(self.conv1(x)))\n","        x = F.relu(self.conv1(x))\n","\n","        if self.feature_transform:\n","            trans_feat = self.fstn(x)\n","            x = x.transpose(2, 1)\n","            x = torch.bmm(x, trans_feat)\n","            x = x.transpose(2, 1)\n","        else:\n","            trans_feat = None\n","\n","        pointfeat = x\n","        # x = F.relu(self.bn2(self.conv2(x)))\n","        # x = self.bn3(self.conv3(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.conv3(x)\n","        x = torch.max(x, 2, keepdim=True)[0]\n","        x = x.view(-1, 1024)\n","        if self.global_feat:\n","            return x, trans, trans_feat\n","        else:\n","            x = x.view(-1, 1024, 1).repeat(1, 1, N)\n","            return torch.cat([x, pointfeat], 1), trans, trans_feat\n","\n","\n","def feature_transform_reguliarzer(trans):\n","    d = trans.size()[1]\n","    I = torch.eye(d)[None, :, :]\n","    if trans.is_cuda:\n","        I = I.cuda()\n","    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2, 1) - I), dim=(1, 2)))\n","    return loss\n","\n","print(\"model call\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["model call\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qk4iVq8bxrNk","executionInfo":{"status":"ok","timestamp":1622122056978,"user_tz":-480,"elapsed":12,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}},"outputId":"8cadc40c-2f79-4be3-8915-ff59e5cbcadb"},"source":["# the whole model network for regression task\n","import torch.nn as nn\n","import torch.utils.data\n","import torch.nn.functional as F\n","\n","class get_model(nn.Module):\n","    def __init__(self, k=40, normal_channel=True):\n","        super(get_model, self).__init__()\n","        if normal_channel:\n","            channel = 7\n","        else:\n","            channel = 3\n","        channel = 3\n","        print(channel)\n","\n","        self.feat = PointNetEncoder(global_feat=True, feature_transform=True, channel=channel)\n","        self.fc1 = nn.Linear(1024, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, k)\n","     \n","        self.dropout = nn.Dropout(p=0.2)\n","\n","    def forward(self, x):\n","        x, trans, trans_feat = self.feat(x)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.dropout(self.fc2(x)))\n","        x = self.fc3(x)\n","        # pdb.set_trace()\n","        # x = F.log_softmax(x, dim=1)\n","\n","\n","        return x, trans_feat\n","\n","class get_loss(torch.nn.Module):\n","    def __init__(self, mat_diff_loss_scale=0.001):\n","        super(get_loss, self).__init__()\n","        self.mat_diff_loss_scale = mat_diff_loss_scale\n","\n","    def forward(self, pred, target, trans_feat):\n","        # loss = F.nll_loss(pred, target)\n","        # target = (2000 -target) / 2000.\n","#         pdb.set_trace()\n","        loss = F.mse_loss(pred, target)\n","#         loss = F.smooth_l1_loss(pred, target)\n","\n","\n","        mat_diff_loss = feature_transform_reguliarzer(trans_feat)\n","\n","#         total_loss = loss + mat_diff_loss * self.mat_diff_loss_scale\n","        # pdb.set_trace()\n","        total_loss = loss\n","        return total_loss\n","print(\"load metric\")"],"execution_count":26,"outputs":[{"output_type":"stream","text":["load metric\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jCiVjkS6xrNk","executionInfo":{"status":"ok","timestamp":1622122058964,"user_tz":-480,"elapsed":1435,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}},"outputId":"59e0a076-f792-4564-b89c-c29ce265cde2"},"source":["'''HYPER PARAMETER'''\n","import datetime\n","from pathlib import Path\n","import logging\n","\n","\n","def log_string(str):\n","    logger.info(str)\n","    print(str)\n","    \n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n","log_dir = 'rgn_point'\n","model = 'pointnet_reg_sml1BN1_point'\n","BATCH_SIZE = 1\n","epoch = 200\n","learning_rate = 0.0001\n","decay_rate = 0.7\n","step_size = 5\n","gpu = '0'\n","optimizer = 'Adam' # 'SGD'\n","lr_decay = 0.7\n","NUM_POINT, Block_size = 500, 2 # doesn't matter now\n","\n","\n","'''CREATE DIR'''\n","timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n","experiment_dir = Path('./log/')\n","experiment_dir.mkdir(exist_ok=True)\n","experiment_dir = experiment_dir.joinpath('reg')\n","experiment_dir.mkdir(exist_ok=True)\n","if log_dir is None:\n","    experiment_dir = experiment_dir.joinpath(timestr)\n","else:\n","    experiment_dir = experiment_dir.joinpath(log_dir)\n","experiment_dir.mkdir(exist_ok=True)\n","checkpoints_dir = experiment_dir.joinpath('checkpoints/')\n","checkpoints_dir.mkdir(exist_ok=True)\n","log_dir = experiment_dir.joinpath('logs/')\n","log_dir.mkdir(exist_ok=True)\n","\n","'''LOG'''\n","\n","logger = logging.getLogger(\"Model\")\n","logger.setLevel(logging.INFO)\n","formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, model))\n","file_handler.setLevel(logging.INFO)\n","file_handler.setFormatter(formatter)\n","logger.addHandler(file_handler)\n","log_string('PARAMETER ...')\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["PARAMETER ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"e5btbbBjxrNl","executionInfo":{"status":"error","timestamp":1622122058995,"user_tz":-480,"elapsed":64,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}},"outputId":"30bca86b-1f58-422f-c211-20678f5e9a56"},"source":["## load data\n","from torch.utils.tensorboard import SummaryWriter\n","import torch\n","import logging\n","import sys\n","import importlib\n","import shutil\n","from tqdm import tqdm\n","# import provider\n","import numpy as np\n","import time\n","%ls \n","root = './data/train_test_split_v2/train_set_txt'\n","val_root = './data/train_test_split_v2/val_set_txt'\n","test_root = './data/train_test_split_v2/test_set_txt'\n","\n","TRAIN_DATASET = GeoData_crop_1(split='train', data_root=root, num_point=NUM_POINT,  block_size=Block_size, transform=None)\n","print(\"start loading test data ...\")\n","VAL_DATASET = GeoData_crop_1(split='test', data_root=val_root, num_point=NUM_POINT,  block_size=Block_size, transform=None)\n","TEST_DATASET = GeoData_crop_1(split='test', data_root=test_root, num_point=NUM_POINT,  block_size=Block_size, transform=None)\n","\n","trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True, drop_last=True, worker_init_fn = lambda x: np.random.seed(x+int(time.time())))\n","valDataLoader = torch.utils.data.DataLoader(VAL_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)\n","testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["AGB_reg_pointnet.ipynb  \u001b[0m\u001b[01;34mmodels\u001b[0m/       test_reg_igndata_block_BN1_point.py\n","\u001b[01;34mdata\u001b[0m/                   provider.py   test_reg_igndata_block_BN1_point_rgb.py\n","\u001b[01;34mdata_utils\u001b[0m/             \u001b[01;34m__pycache__\u001b[0m/  train_reg_igndata_block_BN1_point.py\n","\u001b[01;34mlog\u001b[0m/                    \u001b[01;34mscripts\u001b[0m/      train_reg_igndata_block_BN1_point_rgb.py\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-015f12d0e5ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/train_test_split_v2/test_set_txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mTRAIN_DATASET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeoData_crop_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_POINT\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBlock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start loading test data ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mVAL_DATASET\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeoData_crop_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_POINT\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBlock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-e0688c6def29>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, split, data_root, num_point, block_size, transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwhole_coord_min\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mroom_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'whole_coord_min' is not defined"]}]},{"cell_type":"code","metadata":{"id":"J02nua_KxrNl","executionInfo":{"status":"aborted","timestamp":1622122058968,"user_tz":-480,"elapsed":20,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["# load model and training\n","classifier = get_model(1).cuda()\n","# load metric\n","criterion = get_loss().cuda()\n","from sklearn.metrics import r2_score\n","\n","# weight init\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv2d') != -1:\n","        torch.nn.init.xavier_normal_(m.weight.data)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","    elif classname.find('Linear') != -1:\n","        torch.nn.init.xavier_normal_(m.weight.data)\n","        torch.nn.init.constant_(m.bias.data, 0.0)\n","\n","classifier = classifier.apply(weights_init)\n","\n","# chose optimizer\n","if optimizer == 'Adam':\n","    optimizer = torch.optim.Adam(\n","        classifier.parameters(),\n","        lr=learning_rate,\n","        betas=(0.9, 0.999),\n","        eps=1e-08,\n","        weight_decay=decay_rate\n","    )\n","else:\n","    optimizer = torch.optim.SGD(classifier.parameters(), lr=learning_rate, momentum=0.9)\n","\n","def bn_momentum_adjust(m, momentum):\n","    if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n","        m.momentum = momentum\n","        \n","## define the training parameter\n","\n","LEARNING_RATE_CLIP = 1e-7\n","MOMENTUM_ORIGINAL = 0.1\n","MOMENTUM_DECCAY = 0.5\n","MOMENTUM_DECCAY_STEP = step_size\n","\n","print('load model and training')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzhCmLS3xrNm","executionInfo":{"status":"aborted","timestamp":1622122058972,"user_tz":-480,"elapsed":24,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["## start training\n","start_epoch = 0\n","global_epoch = 0\n","best_loss = 99999\n","\n","TIMESTAMP = \"{0:%Y-%m-%dT%H-%M-%S/}\".format(datetime.datetime.now())\n","writer = SummaryWriter(str(experiment_dir) + \"/tensorboard/\" + TIMESTAMP)\n","for epoch in range(start_epoch, epoch):\n","    log_string('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, epoch))\n","    lr = max(learning_rate * (lr_decay ** (epoch // step_size)), LEARNING_RATE_CLIP)\n","    log_string('Learning rate:%f' % lr)\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr\n","    momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n","    if momentum < 0.01:\n","        momentum = 0.01\n","    print('BN momentum updated to: %f' % momentum)\n","    classifier = classifier.apply(lambda x: bn_momentum_adjust(x,momentum))\n","    num_batches = len(trainDataLoader)\n","    total_correct = 0\n","    total_seen = 0\n","    loss_sum = 0\n","    \n","    ### load batches\n","    for i, data in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n","        points, target = data\n","        points = points.data.numpy()\n","        # points[:,:, :3] = provider.rotate_point_cloud_z(points[:,:, :3])\n","\n","        target = target.data.numpy()\n","        target = torch.Tensor(target)\n","\n","        points = torch.Tensor(points)\n","        points, target = points.float().cuda(),target.float().cuda()\n","        points = points.transpose(2, 1)\n","        optimizer.zero_grad()\n","\n","        classifier = classifier.train()\n","        reg_pred, trans_feat = classifier(points)\n","        reg_pred = reg_pred.contiguous().view(-1)\n","\n","        # batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n","        # target = target.view(-1, 1)[:, 0]\n","        target = target[:, 0]\n","        # pdb.set_trace()\n","\n","        loss = criterion(reg_pred, target.float(), trans_feat)\n","        loss.backward()\n","        optimizer.step()\n","        loss_sum += loss\n","    avl_loss = loss_sum/num_batches\n","\n","    writer.add_scalar('Train_Loss', avl_loss, epoch)\n","    writer.add_scalar('learning_rate',lr , epoch)\n","\n","\n","    log_string('==========Training mean loss: %f ================' % (avl_loss))\n","    \n","    ## save models\n","    if epoch % 50 == 0:\n","        logger.info('Save model...')\n","        # pdb.set_trace()\n","        savepath = str(checkpoints_dir) + '/' + str(epoch) + '_model.pth'\n","        log_string('Saving at %s' % savepath)\n","        state = {\n","            'epoch': epoch,\n","            'model_state_dict': classifier.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","        }\n","        torch.save(state, savepath)\n","        log_string('Saving model....')\n","\n","        \n","    '''Evaluate on validate data'''\n","    with torch.no_grad():\n","        num_batches = len(valDataLoader)\n","        # total_correct = 0\n","        # total_seen = 0\n","        loss_sum = 0\n","        log_string('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n","        for i, data in tqdm(enumerate(valDataLoader), total=len(valDataLoader), smoothing=0.9):\n","            points, target = data\n","            points = points.data.numpy()\n","            points = torch.Tensor(points)\n","            # target to tensor\n","            target = target.data.numpy()\n","            target = torch.Tensor(target)\n","            points, target = points.float().cuda(), target.float().cuda()\n","            points = points.transpose(2, 1)\n","            classifier = classifier.eval()\n","            reg_pred, trans_feat = classifier(points)\n","            reg_pred = reg_pred.contiguous().view(-1)\n","            batch_label = target.cpu().data.numpy()\n","\n","            target = target[:, 0]\n","\n","            loss = criterion(reg_pred, target.float(), trans_feat)\n","            loss_sum += loss\n","        avl_loss = loss_sum/num_batches\n","        log_string('=================== Test mean loss: %f ==============' % (avl_loss))\n","\n","        writer.add_scalar('Val_Loss', avl_loss, epoch)\n","\n","        # log_string('eval mean loss: %f' % (loss_sum / float(num_batches)))\n","        if avl_loss <= best_loss:\n","            best_loss = avl_loss\n","            best_epoch = epoch\n","            logger.info('Save model...')\n","            savepath = str(checkpoints_dir) + '/best_model.pth'\n","            log_string('Saving at %s' % savepath)\n","            state = {\n","                'epoch': epoch,\n","                'class_avg_iou': loss_sum,\n","                'model_state_dict': classifier.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","            }\n","            torch.save(state, savepath)\n","            log_string('Saving model....')\n","        log_string('Best loss: %f' % best_loss)\n","        log_string('Best loss epoch: %f' % best_epoch)\n","\n","    ## new add\n","    '''Evaluate on test data and compute the r2-scores'''\n","    r2_score_sum = 0\n","\n","    with torch.no_grad():\n","        num_batches = len(testDataLoader)\n","        # total_correct = 0\n","        # total_seen = 0\n","        loss_sum = 0\n","        log_string('---- EPOCH %03d test data ----' % (global_epoch + 1))\n","        target_all = list()\n","        reg_all = list()\n","        for i, data in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n","            points, target = data\n","            points = points.data.numpy()\n","            points = torch.Tensor(points)\n","            # target to tensor\n","            target = target.data.numpy()\n","            target = torch.Tensor(target)\n","            points, target = points.float().cuda(), target.float().cuda()\n","            points = points.transpose(2, 1)\n","            classifier = classifier.eval()\n","            reg_pred, trans_feat = classifier(points)\n","            reg_pred = reg_pred.contiguous().view(-1)\n","            batch_label = target.cpu().data.numpy()\n","            target = target[:, 0].float()\n","            loss = criterion(reg_pred, target.float(), trans_feat)\n","            loss_sum += loss\n","\n","            target_all.append(target.cpu())\n","            reg_all.append(reg_pred.cpu())\n","\n","        r2_score_all = r2_score(target_all, reg_all)\n","\n","        avl_loss = loss_sum/num_batches\n","        log_string('=================== Test mean loss: %f ==============' % (avl_loss))\n","        log_string('=================== Test r2_score_all: %f ==============' % (r2_score_all))\n","\n","        writer.add_scalar('Test_Loss', avl_loss, epoch)\n","        writer.add_scalar('Test_r2_scores',r2_score_all , epoch)\n","\n","    global_epoch += 1\n","writer.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smQMdraGxrNn","executionInfo":{"status":"aborted","timestamp":1622122058982,"user_tz":-480,"elapsed":34,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["# view the training process\n","#!tensorboard --logdir=log/reg/rgn_point/tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bjZQ2v3xrNo","executionInfo":{"status":"aborted","timestamp":1622122058985,"user_tz":-480,"elapsed":37,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["## testing the trained model\n","import os\n","import torch\n","import datetime\n","import logging\n","from pathlib import Path\n","import sys\n","from tqdm import tqdm\n","import provider\n","import numpy as np\n","import time\n","from sklearn.metrics import r2_score\n","\n","\n","test_root = '/data/REASEARCH/DEEPCROP/PointCloudData/regression/train_test_split_v2/test_set_txt'\n","TEST_DATASET = GeoData_crop_1(split='test', data_root=test_root, num_point=NUM_POINT,  block_size=Block_size, transform=None)\n","testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True, drop_last=True)\n","\n","# load model and training\n","classifier = get_model(1).cuda()\n","# load metric\n","criterion = get_loss().cuda()\n","checkpoint = torch.load(str(experiment_dir) + '/checkpoints/best_model.pth')\n","\n","start_epoch = checkpoint['epoch']\n","classifier.load_state_dict(checkpoint['model_state_dict'])\n","\n","for i, data in tqdm(enumerate(testDataLoader), total=len(testDataLoader), smoothing=0.9):\n","    points, target = data\n","    points = points.data.numpy()\n","    points = torch.Tensor(points)\n","    # target to tensor\n","    target = target.data.numpy()\n","    target = torch.Tensor(target)\n","    points, target = points.float().cuda(), target.float().cuda()\n","    points = points.transpose(2, 1)\n","    classifier = classifier.eval()\n","    seg_pred, trans_feat = classifier(points)\n","    # pred_val = seg_pred.contiguous().cpu().data.numpy()\n","    seg_pred = seg_pred.contiguous().view(-1)\n","    batch_label = target.cpu().data.numpy()\n","    # target = target.view(-1, 1)[:, 0]\n","\n","    target = target[:, 0]\n","\n","    loss = criterion(seg_pred, target.float(), trans_feat)\n","    loss_sum += loss\n","\n","    target_all.append(target.cpu())\n","    reg_all.append(seg_pred.cpu())\n","\n","r2_score_all = r2_score(target_all, reg_all)\n","\n","# avl_r2_score = r2_score_sum/num_batches\n","avl_loss = loss_sum/num_batches\n","log_string('===================  Test mean L2 loss: %f  ==============' % (avl_loss))\n","log_string('===================  Test mean r2 score: %f  ==============' % (r2_score_all))\n","log_string('===================  Test mean L1 loss: %f  ==============' % (avl_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dagaP61KxrNo","executionInfo":{"status":"aborted","timestamp":1622122058987,"user_tz":-480,"elapsed":38,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["##########  use terminal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZr-ZKPdxrNo","executionInfo":{"status":"aborted","timestamp":1622122058988,"user_tz":-480,"elapsed":39,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["!sh scripts/Train_ign_point.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1NjrjlixrNo","executionInfo":{"status":"aborted","timestamp":1622122058990,"user_tz":-480,"elapsed":41,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["!sh scripts_0526/Test_point.sh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15AT9k6RxrNp","executionInfo":{"status":"aborted","timestamp":1622122058992,"user_tz":-480,"elapsed":43,"user":{"displayName":"Lei Li","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFf7JpbOORQBCai0IogBBLdUpG9Wd_3WHcGtoh=s64","userId":"05036957997681271372"}}},"source":["#!tensorboard --logdir=log/reg/rgn_point/tensorboard"],"execution_count":null,"outputs":[]}]}